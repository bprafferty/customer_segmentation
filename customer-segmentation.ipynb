{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Customer Segmentation\n\n#### By: Brian Rafferty\n\n## Introduction\n\n#### In this project, I will perform unsupervised learning on a supermarket's dataset containing customer information to group individuals that share characteristics. Unsupervised grouping of a dataset is called clustering, and it is extremely beneficial for businesses that want to learn more about the nuances of their data. In the context of this problem, clustering will provide stakeholders of the supermarket with an opportunity to increase revenue in future quarters by knowing which groups of customers will likely have a positive response to marketing campaigns.\n\n## Table of Contents\n\n### 1. Import Libraries\n### 2. Load Data\n### 3. Exploratory Data Analysis\n### 4. Data Cleaning\n### 5. Principle Component Analysis\n### 6. Clustering\n### 7. Profiling\n### 8. Conclusion\n\n## Import Libraries","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-16T08:25:02.312604Z","iopub.execute_input":"2022-02-16T08:25:02.312883Z","iopub.status.idle":"2022-02-16T08:25:02.32569Z","shell.execute_reply.started":"2022-02-16T08:25:02.312854Z","shell.execute_reply":"2022-02-16T08:25:02.32456Z"}}},{"cell_type":"code","source":"import numpy as np\nnp.random.seed(4)\nimport pandas as pd\npd.set_option('max_columns', None)\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt, numpy as np\nplt.style.use('ggplot')\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import metrics\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:02.975220Z","iopub.execute_input":"2022-03-10T19:25:02.975602Z","iopub.status.idle":"2022-03-10T19:25:05.067511Z","shell.execute_reply.started":"2022-03-10T19:25:02.975491Z","shell.execute_reply":"2022-03-10T19:25:05.066498Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/customer-personality-analysis/marketing_campaign.csv', sep='\\t')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.069525Z","iopub.execute_input":"2022-03-10T19:25:05.069785Z","iopub.status.idle":"2022-03-10T19:25:05.114809Z","shell.execute_reply.started":"2022-03-10T19:25:05.069754Z","shell.execute_reply":"2022-03-10T19:25:05.113690Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis\n\n#### Here I will go through the dataset to learn more about it. What I learn here will influence the steps I must take in the data cleaning process. Things that are important to understand before moving forward are: \n#### 1. Data Shape\n#### 2. Data Types\n#### 3. Distribution\n#### 4. Data Missing","metadata":{}},{"cell_type":"code","source":"print(\"Data Shape\\n-----------------\\n# of Rows: {}\\n# of Columns: {}\".format(df.shape[0], df.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.116268Z","iopub.execute_input":"2022-03-10T19:25:05.116766Z","iopub.status.idle":"2022-03-10T19:25:05.122670Z","shell.execute_reply.started":"2022-03-10T19:25:05.116728Z","shell.execute_reply":"2022-03-10T19:25:05.121396Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Use .info() to see the datatype for each column\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.124728Z","iopub.execute_input":"2022-03-10T19:25:05.125348Z","iopub.status.idle":"2022-03-10T19:25:05.169140Z","shell.execute_reply.started":"2022-03-10T19:25:05.125308Z","shell.execute_reply":"2022-03-10T19:25:05.167726Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.171023Z","iopub.execute_input":"2022-03-10T19:25:05.171869Z","iopub.status.idle":"2022-03-10T19:25:05.281049Z","shell.execute_reply.started":"2022-03-10T19:25:05.171821Z","shell.execute_reply":"2022-03-10T19:25:05.280020Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.282665Z","iopub.execute_input":"2022-03-10T19:25:05.283013Z","iopub.status.idle":"2022-03-10T19:25:05.297035Z","shell.execute_reply.started":"2022-03-10T19:25:05.282978Z","shell.execute_reply":"2022-03-10T19:25:05.295493Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"##### Notes: The dataset contains 29 columns and 2240 rows. Out of the 29 columns, only Education, and Maritial_Status require encoding during the data cleaning process. Many columns contain skewed distributions, which I will need to correct during the data cleaning process, those columns include: Income, MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds, NumDealsPurchases, NumWebPurchases, NumCatalogPurchases, and NumWebVisitsMonth. Z_CostContact and Z_Revenue only contain a single value for every row, making those columns useless and in need of dropping during the data cleaning process. Lastly, Income is missing values in 24 rows, so I will drop those rows from the dataset in the data cleaning process as well.\n\n## Data Cleaning\n\n#### To properly clean the dataset, my workflow will entail:\n#### 1. Remove rows with missing values in the Income column\n#### 2. Remove outliers from columns with skewed distributions\n#### 3. Encode columns with string data types into integers ","metadata":{}},{"cell_type":"code","source":"# drop rows with missing values for Income\ndf = df.dropna(axis=0)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.298856Z","iopub.execute_input":"2022-03-10T19:25:05.301238Z","iopub.status.idle":"2022-03-10T19:25:05.319613Z","shell.execute_reply.started":"2022-03-10T19:25:05.301163Z","shell.execute_reply":"2022-03-10T19:25:05.318745Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# drop columns with useless data\ndf.drop(['Dt_Customer', 'Z_CostContact', 'Z_Revenue'], axis=1, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.320984Z","iopub.execute_input":"2022-03-10T19:25:05.321296Z","iopub.status.idle":"2022-03-10T19:25:05.334939Z","shell.execute_reply.started":"2022-03-10T19:25:05.321249Z","shell.execute_reply":"2022-03-10T19:25:05.333848Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# remove outliers from columns with extreme outliers\n#df = df[(np.abs(stats.zscore(df[['Year_Birth', 'Income']])) < 3).all(axis=1)]\ndf = df[df['Income'] < 200000]\ndf = df[df['Year_Birth'] > 1920]\ndf.shape\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.336437Z","iopub.execute_input":"2022-03-10T19:25:05.337406Z","iopub.status.idle":"2022-03-10T19:25:05.372326Z","shell.execute_reply.started":"2022-03-10T19:25:05.337357Z","shell.execute_reply":"2022-03-10T19:25:05.371286Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# do some feature engineering\n\n# find total amount spent at stores\ncol_list = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\ndf['TotalSpent'] = df[col_list].sum(axis = 1)\n\n# find family size\ndf.loc[df['Marital_Status'].isin(['Alone', 'Absurd', 'YOLO', 'Divorced', 'Widow', 'Single']), 'Marital_Status'] = 1\ndf.loc[df['Marital_Status'].isin(['Together', 'Married']), 'Marital_Status'] = 2\n\n# encode string columns to ints\ndf['Education'] = df['Education'].astype('category').cat.codes\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.375694Z","iopub.execute_input":"2022-03-10T19:25:05.376540Z","iopub.status.idle":"2022-03-10T19:25:05.416380Z","shell.execute_reply.started":"2022-03-10T19:25:05.376485Z","shell.execute_reply":"2022-03-10T19:25:05.415242Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# make copy of dataframe keeping metrics: 'AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','AcceptedCmp1','AcceptedCmp2','Response','Complain'\norig_df = df.copy()\ndf.drop(['AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','AcceptedCmp1',\n         'AcceptedCmp2','Response','Complain','ID'], axis=1, inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.417854Z","iopub.execute_input":"2022-03-10T19:25:05.418366Z","iopub.status.idle":"2022-03-10T19:25:05.443771Z","shell.execute_reply.started":"2022-03-10T19:25:05.418325Z","shell.execute_reply":"2022-03-10T19:25:05.442965Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# scale all remaining columns\nscaler = StandardScaler()\nscaler.fit_transform(df)\ndf = pd.DataFrame(scaler.transform(df), columns = df.columns)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.445127Z","iopub.execute_input":"2022-03-10T19:25:05.445899Z","iopub.status.idle":"2022-03-10T19:25:05.499082Z","shell.execute_reply.started":"2022-03-10T19:25:05.445862Z","shell.execute_reply":"2022-03-10T19:25:05.498019Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Principle Component Analysis\n\n#### There are far too many columns in the dataset to effectively apply a clustering algorithm, so I will apply a dimensionality reduction technique called Principle Component Analysis (PCA).","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=3)\npca.fit(df)\npca_df = pd.DataFrame(pca.transform(df), columns=([\"col1\",\"col2\", \"col3\"]))\npca_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.500387Z","iopub.execute_input":"2022-03-10T19:25:05.500630Z","iopub.status.idle":"2022-03-10T19:25:05.581755Z","shell.execute_reply.started":"2022-03-10T19:25:05.500599Z","shell.execute_reply":"2022-03-10T19:25:05.579459Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Clustering\n\n#### Using the reduced dimensions provided by PCA, I will now cluster the dataset and assign the results to the original dataset for future profiling. In order to cluster the data, I will first employ the Elbow Method to determine the optimal number of clusters for this dataset. Afterwards I will apply a hierarchical clustering method so that my results are reproducible. Lastly, I will visualize the clusters that I produced with a 3D scatter plot.","metadata":{}},{"cell_type":"code","source":"elbow = KElbowVisualizer(KMeans(), k=10)\nelbow.fit(pca_df)\nelbow.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:05.585773Z","iopub.execute_input":"2022-03-10T19:25:05.593692Z","iopub.status.idle":"2022-03-10T19:25:22.659402Z","shell.execute_reply.started":"2022-03-10T19:25:05.593561Z","shell.execute_reply":"2022-03-10T19:25:22.658703Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"#### The optimal number of clusters for the dataset is 4.\n\n#### Now I cluster the dataset using Agglomerative Clustering (a type of hierarchical clustering) to generate a cluster prediction for each row in the PCA dataset. I will then connect those clusters directly to the original dataset so that I can conduct profiling in the future.","metadata":{}},{"cell_type":"code","source":"#Initiating the Agglomerative Clustering model \nac = AgglomerativeClustering(n_clusters=4)\n# fit model and predict clusters\npredictions = ac.fit_predict(pca_df)\npca_df[\"Clusters\"] = predictions\norig_df[\"Clusters\"]= predictions","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:22.663723Z","iopub.execute_input":"2022-03-10T19:25:22.665695Z","iopub.status.idle":"2022-03-10T19:25:22.886951Z","shell.execute_reply.started":"2022-03-10T19:25:22.665641Z","shell.execute_reply":"2022-03-10T19:25:22.886261Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### With the clusters generated, now I will visualize the results in a 3D scatterplot.","metadata":{}},{"cell_type":"code","source":"#Plotting the clusters\nx = pca_df[\"col1\"]\ny = pca_df[\"col2\"]\nz = pca_df[\"col3\"]\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=pca_df[\"Clusters\"], marker='o')\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:22.890856Z","iopub.execute_input":"2022-03-10T19:25:22.891298Z","iopub.status.idle":"2022-03-10T19:25:23.364602Z","shell.execute_reply.started":"2022-03-10T19:25:22.891265Z","shell.execute_reply":"2022-03-10T19:25:23.363850Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Profiling\n\n#### With each row in the dataset placed into a cluster, I will begin profiling the results (determine the different characteristics of each cluster).","metadata":{}},{"cell_type":"code","source":"colors = ['#a6cee3','#1f78b4','#b2df8a','#33a02c']\npl = sns.countplot(x=orig_df[\"Clusters\"], palette=colors)\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:23.365509Z","iopub.execute_input":"2022-03-10T19:25:23.365712Z","iopub.status.idle":"2022-03-10T19:25:23.732394Z","shell.execute_reply.started":"2022-03-10T19:25:23.365687Z","shell.execute_reply":"2022-03-10T19:25:23.731676Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"pl = sns.scatterplot(data = orig_df,x=orig_df[\"TotalSpent\"], y=orig_df[\"Income\"],hue=orig_df[\"Clusters\"], palette= colors)\npl.set_title(\"Cluster's Profile Based On Income And Total Spending\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:23.733937Z","iopub.execute_input":"2022-03-10T19:25:23.734213Z","iopub.status.idle":"2022-03-10T19:25:24.200101Z","shell.execute_reply.started":"2022-03-10T19:25:23.734162Z","shell.execute_reply":"2022-03-10T19:25:24.199218Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"plt.figure()\npl=sns.boxenplot(x=orig_df[\"Clusters\"], y=orig_df[\"TotalSpent\"], palette=colors)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:25:24.201333Z","iopub.execute_input":"2022-03-10T19:25:24.201588Z","iopub.status.idle":"2022-03-10T19:25:24.454465Z","shell.execute_reply.started":"2022-03-10T19:25:24.201558Z","shell.execute_reply":"2022-03-10T19:25:24.453779Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"orig_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:31:05.066930Z","iopub.execute_input":"2022-03-10T19:31:05.067271Z","iopub.status.idle":"2022-03-10T19:31:05.089348Z","shell.execute_reply.started":"2022-03-10T19:31:05.067236Z","shell.execute_reply":"2022-03-10T19:31:05.088607Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### Cluster 1 is our star customer group; They spend the most money at the store.","metadata":{}},{"cell_type":"code","source":"characteristics = [ \"Kidhome\",\"Teenhome\", \"Year_Birth\", \"Education\", \"Marital_Status\"]\n\nfor i in characteristics:\n    plt.figure()\n    sns.jointplot(x=orig_df[i], y=orig_df[\"TotalSpent\"], hue =orig_df[\"Clusters\"], kind=\"kde\", palette=colors)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T19:33:45.171173Z","iopub.execute_input":"2022-03-10T19:33:45.172095Z","iopub.status.idle":"2022-03-10T19:33:56.524930Z","shell.execute_reply.started":"2022-03-10T19:33:45.172050Z","shell.execute_reply":"2022-03-10T19:33:56.523799Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"#### Cluster 0: \n* Most are parents\n* Most are younger\n* Wide range of educations\n* Most are married\n\n#### Cluster 1:\n* Not parents\n* Wide range of ages\n* Most have high education\n* Half are married\n\n#### Cluster 2:\n* Most are parents\n* Most are older\n* Most have high education\n* Most are married\n\n#### Cluster 3:\n* Most are parents\n* Most are older\n* Most have high education\n* Half are married","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\n#### Using unsupervised learning to segment customers, I was able to learn that individuals who are not parents and have high education are most likely to purchase products at the store. This group of people should be leveraged with future marketing campaigns to maximize profit.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}